name: main

on:
  # Runs every 10 minutes
  schedule:
    - cron: '*/10 * * * *'
  # Allows you to run it manually from the Actions tab for testing
  workflow_dispatch:

jobs:
  scrape-and-commit:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checks out your repository's code so the action can access it
      - name: Checkout repo
        uses: actions/checkout@v4

      # Step 2: Sets up the Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10' # Or your preferred version

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests

      # Step 3: Run your Python script that appends data
      - name: Run scraper to append data
        run: python entergy_scrapper.py # <-- IMPORTANT: Change to your script's filename

      # Step 4: Commit the updated data file back to the repository
      # THIS IS THE CRITICAL STEP YOU ARE MISSING
      - name: Commit and push if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          # The commit message that will be used
          commit_message: "chore(data): Auto-update Louisiana outage data"

          # The specific file to look for changes in.
          # This MUST match the DESTINATION_FILE in your Python script.
          file_pattern: 'data/louisiana-county.csv'

          # The user name and email for the commit
          commit_user_name: "GitHub Actions Bot"
          commit_user_email: "actions@github.com"
